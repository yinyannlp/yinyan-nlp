{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "778955it [02:32, 5107.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778955, 30) (778955, 30) (778955, 30) (778955,)\n",
      "(620000, 30) (620000,)\n",
      "[628746 745702 605667 118057 179863  36038 769851 133383 510526  34092]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:/Code/bert-first-test/bert-model/ were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:102275338,Trainable parameters:102275338\n",
      " Running training epoch 1\n",
      "Epoch 0001 | Step 1937/9688 | Loss0.8135 | Time 586.6170\n",
      "Epoch 0001 | Step 3874/9688 | Loss0.5141 | Time 1179.1316\n",
      "Epoch 0001 | Step 5811/9688 | Loss0.4036 | Time 1767.4159\n",
      "Epoch 0001 | Step 7748/9688 | Loss0.3445 | Time 2355.9335\n",
      "Epoch 0001 | Step 9685/9688 | Loss0.3077 | Time 2925.7020\n",
      "current acc is 0.9490,best acc is 0.9490\n",
      "time costed = 3033.80965s \n",
      "\n",
      " Running training epoch 2\n",
      "Epoch 0002 | Step 1937/9688 | Loss0.1382 | Time 569.8791\n",
      "Epoch 0002 | Step 3874/9688 | Loss0.1385 | Time 1139.6238\n",
      "Epoch 0002 | Step 5811/9688 | Loss0.1370 | Time 1711.1569\n",
      "Epoch 0002 | Step 7748/9688 | Loss0.1358 | Time 2280.8896\n",
      "Epoch 0002 | Step 9685/9688 | Loss0.1346 | Time 2850.6463\n",
      "current acc is 0.9577,best acc is 0.9577\n",
      "time costed = 2958.86644s \n",
      "\n",
      " Running training epoch 3\n",
      "Epoch 0003 | Step 1937/9688 | Loss0.0981 | Time 569.7505\n",
      "Epoch 0003 | Step 3874/9688 | Loss0.0997 | Time 1139.4834\n",
      "Epoch 0003 | Step 5811/9688 | Loss0.1012 | Time 1709.2256\n",
      "Epoch 0003 | Step 7748/9688 | Loss0.1017 | Time 2279.0161\n",
      "Epoch 0003 | Step 9685/9688 | Loss0.1019 | Time 2848.8098\n",
      "current acc is 0.9565,best acc is 0.9577\n",
      "time costed = 2956.13955s \n",
      "\n",
      " Running training epoch 4\n",
      "Epoch 0004 | Step 1937/9688 | Loss0.0745 | Time 569.7251\n",
      "Epoch 0004 | Step 3874/9688 | Loss0.0770 | Time 1139.4155\n",
      "Epoch 0004 | Step 5811/9688 | Loss0.0778 | Time 1709.1363\n",
      "Epoch 0004 | Step 7748/9688 | Loss0.0788 | Time 2278.8620\n",
      "Epoch 0004 | Step 9685/9688 | Loss0.0796 | Time 2848.6283\n",
      "current acc is 0.9578,best acc is 0.9578\n",
      "time costed = 2956.78938s \n",
      "\n",
      " Running training epoch 5\n",
      "Epoch 0005 | Step 1937/9688 | Loss0.0598 | Time 569.8288\n",
      "Epoch 0005 | Step 3874/9688 | Loss0.0602 | Time 1139.6156\n",
      "Epoch 0005 | Step 5811/9688 | Loss0.0605 | Time 1709.3814\n",
      "Epoch 0005 | Step 7748/9688 | Loss0.0614 | Time 2279.1585\n",
      "Epoch 0005 | Step 9685/9688 | Loss0.0622 | Time 2849.0292\n",
      "current acc is 0.9575,best acc is 0.9578\n",
      "time costed = 2956.32661s \n",
      "\n",
      " Running training epoch 6\n",
      "Epoch 0006 | Step 1937/9688 | Loss0.0437 | Time 569.7465\n",
      "Epoch 0006 | Step 3874/9688 | Loss0.0462 | Time 1139.5372\n",
      "Epoch 0006 | Step 5811/9688 | Loss0.0478 | Time 1709.3866\n",
      "Epoch 0006 | Step 7748/9688 | Loss0.0489 | Time 2279.2282\n",
      "Epoch 0006 | Step 9685/9688 | Loss0.0498 | Time 2849.0979\n",
      "current acc is 0.9570,best acc is 0.9578\n",
      "time costed = 2956.49041s \n",
      "\n",
      " Running training epoch 7\n",
      "Epoch 0007 | Step 1937/9688 | Loss0.0345 | Time 569.7995\n",
      "Epoch 0007 | Step 3874/9688 | Loss0.0364 | Time 1139.5101\n",
      "Epoch 0007 | Step 5811/9688 | Loss0.0375 | Time 1709.4323\n",
      "Epoch 0007 | Step 7748/9688 | Loss0.0383 | Time 2279.3433\n",
      "Epoch 0007 | Step 9685/9688 | Loss0.0392 | Time 2849.1642\n",
      "current acc is 0.9574,best acc is 0.9578\n",
      "time costed = 2956.65878s \n",
      "\n",
      " Running training epoch 8\n",
      "Epoch 0008 | Step 1937/9688 | Loss0.0275 | Time 569.8250\n",
      "Epoch 0008 | Step 3874/9688 | Loss0.0286 | Time 1139.6150\n",
      "Epoch 0008 | Step 5811/9688 | Loss0.0299 | Time 1709.3840\n",
      "Epoch 0008 | Step 7748/9688 | Loss0.0305 | Time 2279.1864\n",
      "Epoch 0008 | Step 9685/9688 | Loss0.0310 | Time 2849.0133\n",
      "current acc is 0.9565,best acc is 0.9578\n",
      "time costed = 2956.49996s \n",
      "\n",
      " Running training epoch 9\n",
      "Epoch 0009 | Step 1937/9688 | Loss0.0221 | Time 569.8616\n",
      "Epoch 0009 | Step 3874/9688 | Loss0.0226 | Time 1139.6572\n",
      "Epoch 0009 | Step 5811/9688 | Loss0.0233 | Time 1709.5192\n",
      "Epoch 0009 | Step 7748/9688 | Loss0.0236 | Time 2279.3897\n",
      "Epoch 0009 | Step 9685/9688 | Loss0.0242 | Time 2849.2223\n",
      "current acc is 0.9559,best acc is 0.9578\n",
      "time costed = 2956.59248s \n",
      "\n",
      " Running training epoch 10\n",
      "Epoch 0010 | Step 1937/9688 | Loss0.0178 | Time 569.8847\n",
      "Epoch 0010 | Step 3874/9688 | Loss0.0184 | Time 1155.6587\n",
      "Epoch 0010 | Step 5811/9688 | Loss0.0190 | Time 1755.2518\n",
      "Epoch 0010 | Step 7748/9688 | Loss0.0195 | Time 2347.6165\n",
      "Epoch 0010 | Step 9685/9688 | Loss0.0199 | Time 2944.6687\n",
      "current acc is 0.9570,best acc is 0.9578\n",
      "time costed = 3055.57384s \n",
      "\n",
      " Running training epoch 11\n",
      "Epoch 0011 | Step 1937/9688 | Loss0.0143 | Time 592.8905\n",
      "Epoch 0011 | Step 3874/9688 | Loss0.0145 | Time 1175.2653\n",
      "Epoch 0011 | Step 5811/9688 | Loss0.0147 | Time 1755.9251\n",
      "Epoch 0011 | Step 7748/9688 | Loss0.0150 | Time 2328.6430\n",
      "Epoch 0011 | Step 9685/9688 | Loss0.0152 | Time 2912.6542\n",
      "current acc is 0.9574,best acc is 0.9578\n",
      "time costed = 3024.11339s \n",
      "\n",
      " Running training epoch 12\n",
      "Epoch 0012 | Step 1937/9688 | Loss0.0107 | Time 569.8877\n",
      "Epoch 0012 | Step 3874/9688 | Loss0.0113 | Time 1132.6540\n",
      "Epoch 0012 | Step 5811/9688 | Loss0.0116 | Time 1696.2052\n",
      "Epoch 0012 | Step 7748/9688 | Loss0.0118 | Time 2257.3531\n",
      "Epoch 0012 | Step 9685/9688 | Loss0.0121 | Time 2818.8709\n",
      "current acc is 0.9572,best acc is 0.9578\n",
      "time costed = 2924.70879s \n",
      "\n",
      " Running training epoch 13\n",
      "Epoch 0013 | Step 1937/9688 | Loss0.0082 | Time 567.2092\n",
      "Epoch 0013 | Step 3874/9688 | Loss0.0088 | Time 1134.1375\n",
      "Epoch 0013 | Step 5811/9688 | Loss0.0089 | Time 1700.7457\n",
      "Epoch 0013 | Step 7748/9688 | Loss0.0093 | Time 2279.6464\n",
      "Epoch 0013 | Step 9685/9688 | Loss0.0094 | Time 2843.6873\n",
      "current acc is 0.9574,best acc is 0.9578\n",
      "time costed = 2949.69827s \n",
      "\n",
      " Running training epoch 14\n",
      "Epoch 0014 | Step 1937/9688 | Loss0.0066 | Time 582.0098\n",
      "Epoch 0014 | Step 3874/9688 | Loss0.0070 | Time 1171.9052\n",
      "Epoch 0014 | Step 5811/9688 | Loss0.0071 | Time 1753.2701\n",
      "Epoch 0014 | Step 7748/9688 | Loss0.0073 | Time 2317.5736\n",
      "Epoch 0014 | Step 9685/9688 | Loss0.0074 | Time 2883.1302\n",
      "current acc is 0.9582,best acc is 0.9582\n",
      "time costed = 2990.31544s \n",
      "\n",
      " Running training epoch 15\n",
      "Epoch 0015 | Step 1937/9688 | Loss0.0047 | Time 561.9774\n",
      "Epoch 0015 | Step 3874/9688 | Loss0.0053 | Time 1123.2087\n",
      "Epoch 0015 | Step 5811/9688 | Loss0.0055 | Time 1684.2225\n",
      "Epoch 0015 | Step 7748/9688 | Loss0.0056 | Time 2245.2500\n",
      "Epoch 0015 | Step 9685/9688 | Loss0.0058 | Time 2806.3496\n",
      "current acc is 0.9580,best acc is 0.9582\n",
      "time costed = 2912.08622s \n",
      "\n",
      " Running training epoch 16\n",
      "Epoch 0016 | Step 1937/9688 | Loss0.0040 | Time 561.0802\n",
      "Epoch 0016 | Step 3874/9688 | Loss0.0041 | Time 1122.1682\n",
      "Epoch 0016 | Step 5811/9688 | Loss0.0043 | Time 1683.3781\n",
      "Epoch 0016 | Step 7748/9688 | Loss0.0044 | Time 2244.4352\n",
      "Epoch 0016 | Step 9685/9688 | Loss0.0045 | Time 2805.8054\n",
      "current acc is 0.9583,best acc is 0.9583\n",
      "time costed = 2912.48312s \n",
      "\n",
      " Running training epoch 17\n",
      "Epoch 0017 | Step 1937/9688 | Loss0.0033 | Time 561.1979\n",
      "Epoch 0017 | Step 3874/9688 | Loss0.0032 | Time 1124.1422\n",
      "Epoch 0017 | Step 5811/9688 | Loss0.0034 | Time 1686.4735\n",
      "Epoch 0017 | Step 7748/9688 | Loss0.0035 | Time 2248.5866\n",
      "Epoch 0017 | Step 9685/9688 | Loss0.0035 | Time 2809.6183\n",
      "current acc is 0.9586,best acc is 0.9586\n",
      "time costed = 2916.18025s \n",
      "\n",
      " Running training epoch 18\n",
      "Epoch 0018 | Step 1937/9688 | Loss0.0030 | Time 560.9982\n",
      "Epoch 0018 | Step 3874/9688 | Loss0.0030 | Time 1122.0108\n",
      "Epoch 0018 | Step 5811/9688 | Loss0.0030 | Time 1682.8989\n",
      "Epoch 0018 | Step 7748/9688 | Loss0.0031 | Time 2243.9749\n",
      "Epoch 0018 | Step 9685/9688 | Loss0.0030 | Time 2822.0374\n",
      "current acc is 0.9587,best acc is 0.9587\n",
      "time costed = 2936.32381s \n",
      "\n",
      " Running training epoch 19\n",
      "Epoch 0019 | Step 1937/9688 | Loss0.0026 | Time 568.9544\n",
      "Epoch 0019 | Step 3874/9688 | Loss0.0027 | Time 1134.8936\n",
      "Epoch 0019 | Step 5811/9688 | Loss0.0027 | Time 1702.9066\n",
      "Epoch 0019 | Step 7748/9688 | Loss0.0027 | Time 2271.3928\n",
      "Epoch 0019 | Step 9685/9688 | Loss0.0027 | Time 2865.0735\n",
      "current acc is 0.9588,best acc is 0.9588\n",
      "time costed = 2979.27618s \n",
      "\n",
      " Running training epoch 20\n",
      "Epoch 0020 | Step 1937/9688 | Loss0.0022 | Time 567.3272\n",
      "Epoch 0020 | Step 3874/9688 | Loss0.0024 | Time 1136.3527\n",
      "Epoch 0020 | Step 5811/9688 | Loss0.0024 | Time 1701.7354\n",
      "Epoch 0020 | Step 7748/9688 | Loss0.0024 | Time 2266.7220\n",
      "Epoch 0020 | Step 9685/9688 | Loss0.0024 | Time 2831.9075\n",
      "current acc is 0.9587,best acc is 0.9588\n",
      "time costed = 2938.76025s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json,time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler\n",
    "from transformers import BertModel,BertConfig,BertTokenizer,AdamW,get_cosine_schedule_with_warmup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "bert_path = 'D:/Code/bert-first-test/bert-model/'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "#预处理数据集\n",
    "input_ids,input_masks,input_types, = [],[],[]\n",
    "labels = []\n",
    "maxlen = 30\n",
    "\n",
    "with open('D:/Code/bert-first-test/news.txt','r',encoding='utf-8') as f :\n",
    "    data = f.readlines()\n",
    "    for i,line in tqdm(enumerate(data)):\n",
    "        y,title = line.split(sep=' ',maxsplit=1) \n",
    "        \n",
    "        encode_dict = tokenizer.encode_plus(text = title,add_special_tokens=True , max_length = maxlen,padding = \"max_length\" , truncation = True )\n",
    "        \n",
    "        \n",
    "        input_ids.append(encode_dict['input_ids']) #在词典中的映射\n",
    "        input_masks.append(encode_dict['attention_mask']) #指定对哪些词进行self-Attention操作\n",
    "        input_types.append(encode_dict['token_type_ids']) #句子编号 0 1\n",
    "        \n",
    "       # tokens = tokenizer.tokenize(title)\n",
    "       # print(encode_dict)\n",
    "        labels.append(int(y))\n",
    "\n",
    "input_ids,input_types,input_masks = np.array(input_ids),np.array(input_types),np.array(input_masks)\n",
    "labels = np.array(labels)\n",
    "print(input_ids.shape,input_types.shape,input_masks.shape,labels.shape)\n",
    "\n",
    "idxes = np.arange(input_ids.shape[0]) #文本数量\n",
    "np.random.seed(2022)\n",
    "np.random.shuffle(idxes) #打乱序号\n",
    "input_ids_train, input_ids_valid,input_ids_test = input_ids[idxes[:620000]],input_ids[idxes[620000:700000]],input_ids[idxes[700000:]]\n",
    "input_masks_train, input_masks_valid,input_masks_test = input_masks[idxes[:620000]],input_masks[idxes[620000:700000]],input_masks[idxes[700000:]]\n",
    "input_type_train, input_type_valid,input_type_test = input_types[idxes[:620000]],input_types[idxes[620000:700000]],input_types[idxes[700000:]]\n",
    "y_train,y_valid,y_test = labels[idxes[:620000]],labels[idxes[620000:700000]],labels[idxes[700000:]]\n",
    "print(input_ids_train.shape,y_train.shape)\n",
    "print(idxes[:10])\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "#训练集\n",
    "train_data = TensorDataset(torch.LongTensor(input_ids_train),\n",
    "                           torch.LongTensor(input_masks_train),\n",
    "                           torch.LongTensor(input_type_train),\n",
    "                           torch.LongTensor(y_train)\n",
    "                          )\n",
    "train_sampler = RandomSampler(train_data)# 随机采样训练集\n",
    "train_loader = DataLoader(train_data,sampler = train_sampler,batch_size = BATCH_SIZE)\n",
    "\n",
    "#验证集\n",
    "valid_data = TensorDataset(torch.LongTensor(input_ids_valid),\n",
    "                           torch.LongTensor(input_masks_valid),\n",
    "                           torch.LongTensor(input_type_valid),\n",
    "                           torch.LongTensor(y_valid)                                              \n",
    "                          )\n",
    "valid_sampler = SequentialSampler(valid_data) #顺序采样\n",
    "valid_loader = DataLoader(valid_data, sampler = valid_sampler ,batch_size = BATCH_SIZE)\n",
    "\n",
    "#测试集\n",
    "test_data = TensorDataset(\n",
    "                           torch.LongTensor(input_ids_test),\n",
    "                           torch.LongTensor(input_masks_test),\n",
    "                           torch.LongTensor(input_type_test),\n",
    "                           \n",
    ")\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler = test_sampler ,batch_size = BATCH_SIZE)\n",
    "\n",
    "\n",
    "#定义bert 模型\n",
    "from turtle import forward\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self,bert_path,classes = 10) :\n",
    "        super(Bert_Model,self).__init__()\n",
    "        self.config = BertConfig.from_pretrained(bert_path) #导入模型超参数\n",
    "        self.bert = BertModel.from_pretrained(bert_path)  #加载预训练模型权重\n",
    "        self.fc = nn.Linear(self.config.hidden_size,classes) #多分类 接线性层 可以加一些层数\n",
    "\n",
    "\n",
    "    def forward(self,input_ids,attention_mask = None,token_type_ids = None) :\n",
    "        outputs = self.bert(input_ids,attention_mask,token_type_ids)\n",
    "        out_pool = outputs[1]  #池化后的输出  [batch_size,config_size]\n",
    "        logit = self.fc(out_pool) #[batch_size,classes]\n",
    "        return logit\n",
    "\n",
    "#实例化bert模型\n",
    "def get_parameter_number(model):\n",
    "    #打印模型参数\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return 'Total parameters:{},Trainable parameters:{}'.format(total_num,trainable_num)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 20\n",
    "model = Bert_Model(bert_path).to(DEVICE)\n",
    "print(get_parameter_number(model))\n",
    "\n",
    "#定义优化器\n",
    "optimizer = AdamW(model.parameters(),lr = 2e-5,weight_decay= 1e-4) #Adam优化器\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=len(train_loader),\n",
    "                                            num_training_steps = EPOCHS*len(train_loader))\n",
    "\n",
    "                                            #学习率先线性warmup一个epoch，然后cosine式下降\n",
    "                                            #必须加warmup  不然可能不会收敛\n",
    "                                            \n",
    "#评估模型 在验证集上\n",
    "from statistics import mode\n",
    "\n",
    "\n",
    "def evaluate(model,data_loader,device):\n",
    "    model.eval()\n",
    "    val_true,val_pred = [],[]\n",
    "    with torch.no_grad():\n",
    "        for idx,(ids,att,tpe,y) in (enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device),att.to(device),tpe.to(device))\n",
    "            y_pred = torch.argmax(y_pred,dim = 1 ).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "            val_true.extend(y.squeeze().cpu().numpy().tolist())\n",
    "\n",
    "    return accuracy_score(val_true,val_pred)\n",
    "\n",
    "#测试集没有标签，需要预测提交\n",
    "\n",
    "def predict(model,data_loader,device):\n",
    "    model.eval()\n",
    "    val_pred = []\n",
    "    with torch.no_grad():\n",
    "        for idx,(ids,att,tpe) in tqdm(enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device),att.to(device),tpe.to(device))\n",
    "            y_pred = torch.argmax(y_pred,dim = 1 ).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "    return val_pred\n",
    "\n",
    "\n",
    "def train_and_eval(model,train_loader,valid_loader,optimizer,scheduler,device,epoch):\n",
    "\n",
    "    best_acc = 0.0\n",
    "    patience = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for i in range(epoch):\n",
    "        '''训练模型'''\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        print(\" Running training epoch {}\".format(i+1))\n",
    "        train_loss_sum = 0.0\n",
    "        for idx,(ids,att,tpe,y)in enumerate(train_loader):\n",
    "            ids,att,tpe,y =ids.to(device),att.to(device),tpe.to(device),y.to(device)\n",
    "            y_pred = model(ids,att,tpe)\n",
    "            loss = criterion(y_pred ,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss_sum += loss.item()\n",
    "            if(idx + 1)%(len(train_loader)//5) == 0:\n",
    "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss{:.4f} | Time {:.4f}\".format(\n",
    "                    i+1,idx+1,len(train_loader),train_loss_sum/(idx+1),time.time()-start\n",
    "                ))\n",
    "\n",
    "     #验证模型           \n",
    "        model.eval()\n",
    "        acc = evaluate(model,valid_loader,device)\n",
    "        \n",
    "        if acc> best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(),\"best_bert_model.pth\")\n",
    "        print(\"current acc is {:.4f},best acc is {:.4f}\".format(acc,best_acc))\n",
    "        print(\"time costed = {}s \\n\".format(round(time.time()-start,5)))\n",
    "\n",
    "\n",
    "\n",
    "train_and_eval(model,train_loader,valid_loader,optimizer,scheduler,DEVICE,EPOCHS)     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
